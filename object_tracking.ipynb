{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trung0Minh/AIO2023-MODULE-5/blob/main/object_tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def getFaceLocation():\n",
        "    camera = cv2.VideoCapture(0)\n",
        "    facecascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    ret = True\n",
        "\n",
        "    # Refresh the camera\n",
        "    for i in range(5):\n",
        "        ret, frame = camera.read()\n",
        "    while ret:\n",
        "        ret, frame = camera.read()\n",
        "        if ret:\n",
        "            # Convert the frame to grayscale for face detection\n",
        "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            # Detect faces using Haar Cascade Classifier\n",
        "            faces = facecascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "            # If at least one face is detected, process the first face\n",
        "            if len(faces) > 0:\n",
        "                (x, y, w, h) = faces[0]  # Take the first detected face\n",
        "                face_image = frame[y:y+h, x:x+w]  # Extract the face region\n",
        "                face_location = (x, y, w, h)  # Store the location as (x, y, width, height)\n",
        "                # Release resources and return the face image and location\n",
        "                camera.release()\n",
        "                cv2.destroyAllWindows()\n",
        "                return face_image, face_location\n",
        "        else:\n",
        "            print('error')\n",
        "            break\n",
        "    # If no face is detected or an error occurs, release resources and return None\n",
        "    camera.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    return None, None\n",
        "\n",
        "face_image, face_location = getFaceLocation()"
      ],
      "metadata": {
        "id": "extIqV_ByIb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def face_tracking(template, method):\n",
        "    ret = True\n",
        "    camera = cv2.VideoCapture(0)\n",
        "    # Refresh the camera\n",
        "    for i in range(5):\n",
        "        ret, frame = camera.read()\n",
        "    # Assume template is provided (e.g., from Problem 1); here we initialize with the first frame for demonstration\n",
        "    ret, frame = camera.read()\n",
        "    template = frame[100:200, 100:200]  # Example ROI; in practice, use face_image from Problem 1\n",
        "    template = cv2.cvtColor(template, cv2.COLOR_BGR2GRAY)\n",
        "    method = cv2.TM_CCOEFF_NORMED  # Define the matching method\n",
        "\n",
        "    while ret:\n",
        "        ret, frame = camera.read()\n",
        "        if ret:\n",
        "            img_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "            # Store width and height of template\n",
        "            w, h = template.shape[::-1]\n",
        "            res = cv2.matchTemplate(img_gray, template, method)\n",
        "            # Find the best match location\n",
        "            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res)\n",
        "            # For TM_CCOEFF_NORMED, use max_loc for the best match\n",
        "            if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]:\n",
        "                top_left = min_loc\n",
        "            else:\n",
        "                top_left = max_loc\n",
        "            bottom_right = (top_left[0] + w, top_left[1] + h)\n",
        "            # Draw rectangle around the matched region\n",
        "            cv2.rectangle(frame, top_left, bottom_right, 255, 2)\n",
        "            cv2.imshow('template', template)\n",
        "            cv2.imshow('frames', frame)\n",
        "            # Update template with the matched region, converting to grayscale\n",
        "            cropped = frame[top_left[1]:bottom_right[1], top_left[0]:bottom_right[0]]\n",
        "            template = cv2.cvtColor(cropped, cv2.COLOR_BGR2GRAY)\n",
        "            if cv2.waitKey(1) == 27:  # Exit on ESC key\n",
        "                break\n",
        "    camera.release()\n",
        "    cv2.destroyAllWindows()  # Replace close_window_os() with standard OpenCV function"
      ],
      "metadata": {
        "id": "fpQkLVWIzez7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import argparse\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def object_tracking_video_file():\n",
        "    cap = cv.VideoCapture(\"video.mp4\")  # Replace with actual video path\n",
        "    # Take first frame of the video\n",
        "    ret, frame = cap.read()\n",
        "    # Setup initial location of window\n",
        "    x, y, w, h = (300, 400, 200, 200)  # Hardcoded values\n",
        "    track_window = (x, y, w, h)\n",
        "    # Set up the ROI for tracking\n",
        "    roi = frame[y:y+h, x:x+w]\n",
        "    # Compute histogram and setup for Mean Shift\n",
        "    hsv_roi = cv.cvtColor(roi, cv.COLOR_BGR2HSV)\n",
        "    roi_hist = cv.calcHist([hsv_roi], [0], None, [180], [0, 180])\n",
        "    cv.normalize(roi_hist, roi_hist, 0, 255, cv.NORM_MINMAX)\n",
        "    term_crit = (cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 1)\n",
        "\n",
        "    history = []\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            hsv = cv.cvtColor(frame, cv.COLOR_BGR2HSV)\n",
        "            dst = cv.calcBackProject([hsv], [0], roi_hist, [0, 180], 1)\n",
        "            # Apply Mean Shift to get the new location\n",
        "            ret, track_window = cv.meanShift(dst, track_window, term_crit)\n",
        "            # Draw it on image\n",
        "            x, y, w, h = track_window\n",
        "            img2 = cv.rectangle(frame, (x, y), (x+w, y+h), (255, 255, 0), 4)\n",
        "            history.append((x + int(w/2), y + int(h/2)))\n",
        "            if len(history) >= 2:\n",
        "                for i in range(len(history) - 1):\n",
        "                    (x1, y1) = history[i]\n",
        "                    (x2, y2) = history[i + 1]\n",
        "                    cv.line(img2, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2_imshow(img2)\n",
        "            k = cv.waitKey(30) & 0xff\n",
        "            if k == 27:\n",
        "                break\n",
        "        else:\n",
        "            break\n",
        "    cap.release()\n",
        "    cv.destroyAllWindows()\n",
        "\n",
        "object_tracking_video_file()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BDE0AVbD2aQD",
        "outputId": "f0e66349-80e9-41f2-9198-daa451004c0c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMg4MGWv2V2KiMlVAfSz1x0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}